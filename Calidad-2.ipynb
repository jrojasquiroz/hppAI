{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a4bd8c-7e22-4025-a65b-a23e84c3f9e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. SVM (6k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e576b-a4ba-4bb4-abae-b2f0a7cfa54d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Revisión de la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991574c9-a523-4190-80e1-9e305e832e5d",
   "metadata": {},
   "source": [
    "Con la BBDD que me compartió Carlos, voy a crear un modelo que prediga la calidad de inmuebles a partir de la descripción de venta. Ojo: no uso los datos de Paúl porque no tienen el resultado factorial de Carlos: *Estado_contemporaneidad_calidad* (FAC1_1) y *Ausencia_singulares_presencia_arm_cocina* (FAC2_1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecd57bb-d2d8-4b96-a1e6-3a6360888ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec26856-7564-45c8-a901-ef3120450792",
   "metadata": {},
   "source": [
    "Importamos el archivo, revisamos las columnas, nos quedamos sólo con las que nos interesan y vemos cuántas filas tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08216e5b-ee94-4028-a074-0033a6f732a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h=pd.read_spss('rawdata/BDDHabitaclia_4043_join.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcf9ace-6302-4476-95b1-a6e67b88d0a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OBJECTID_1', 'codigo_inmueble1', 'Title', 'Type_build', 'Type_opera', 'Link', 'Location', 'Lat_X', 'Lon_Y', 'Climatic_Z', 'Nom_Mun', 'precio_eur', 'superficie', 'superficie_2', 'Unit_price', 'Ln_total_pr', 'Ln_unit_pr', 'numero_habitaciones', 'numero_bano', 'ratio_bano_hab', 'numero_aseo', 'ascensor', 'interac_planta', 'numero_de_piso', 'anyo_constr', 'anyo_constr_ponderad', 'antig_ponderad', 'Inverse_Age', 'Year_Before_1981', 'Year_1982_2006', 'Year_After_2007', 'superficie_terraza_m2', 'grand_terr_20m2', 'superficie_jardin_m2', 'superficie_salon', 'bool_despacho', 'bool_buhardilla', 'bool_trastero', 'bool_lavadero', 'bool_piscina_comunitaria', 'bool_jardin_comunitario', 'bool_amueblado', 'bool_ascensor', 'descripcion', 'bool_aire_acondicionado', 'bool_calefaccion', 'bool_chimenea', 'texto_destacado', 'Description', 'calificacion_consumo_letra', 'calificacion_consumo_valor', 'calificacion_emision_letra', 'calificacion_emision_valor', 'Dum_EPC', 'EPC_A_emision', 'EPC_B_emision', 'EPC_C_emision', 'EPC_D_emision', 'EPC_E_emision', 'EPC_F_emision', 'EPC_G_emision', 'COD_MUN', 'temp_enero', 'temp_julio', 'radiacion_enero', 'radiacion_julio', 'POB_91', 'POB_01', 'POB_06', 'POR_01', 'LTL1991_M', 'LTL_2001', 'DLTL_MUN', 'RW', 'FLE', 'FLS', 'SUP_URB_90', 'SUP_URB_00', 'Job_ratio_01', 'Autocontencion_01', 'Nodalidad_01', 'Dist_CBD', 'Dist_CBD2', 'Dist_sub_center', 'Elevation_Mean', 'dum_acces_viappal', 'IND_pr', 'FIRE_pr', 'Div_LandUse', 'COD_SEC', 'pr_directivo', 'pr_tecnico_prof', 'pr_tecnico_apoyo', 'pr_empl_admin', 'pr_restaur_comer', 'pr_agri_calificado', 'pr_artesano', 'pr_operador', 'pr_no_calif', 'desplaz_ponderado', 'plant_ras_pond', 'edif_ruin_pr', 'edif_malo_pr', 'edif_deficient_pr', 'edif_bueno_pr', 'Doorman_pr', 'opin_ruido_si_pr', 'opin_contam_si_pr', 'opin_calle_sucia_pr', 'opin_mala_comunic_pr', 'opin_pocazonaverde_pr', 'opin_delincuencia_pr', 'opin_falta_aseo_pr', 'local_salud_pr', 'local_edu_pr', 'local_social_pr', 'local_cult_pr', 'local_comerc_pr', 'local_oficinas_pr', 'local_industr_pr', 'local_agrar_pr', 'POB_TOTAL', 'POB_RESID', 'LOC_TOTAL', 'POR_TOTAL', 'LOC_VIV_TOTAL', 'dens_loc_100hab', 'dens_loc_sup', 'dens_pob_sup', 'estud_sin_pr', 'estud_primer_pr', 'estud_segund_pr', 'estud_tercer_pr', 'VIV_ppales_TOTAL', 'Sup_viv_sec', 'viv_ppales_pr', 'viv_no_ppales_pr', 'viv_secundarias_pr', 'viv_vacias_pr', 'viv_unifam_pr', 'viv_aptos_pr', 'resi_euro_pr', 'resi_africa_pr', 'resi_america_pr', 'resi_asia_pr', 'resi_oceania_pr', 'H_ocup_POR', 'H_loc_INE', 'H_tamaviv', 'H_ocup_POR_Xpor', 'H_loc_INE_XLOCS', 'H_tamaviv_Xvivs', 'CT_renta_alta_CPA', 'CT_renta_meda_CPA', 'CT_renta_medb_CPA', 'Income_Household_2016', 'DP2e', 'Dens_Time_total', 'Dens_Time_total_work', 'Dens_Time_total_Nwork', 'Dens_pers_act_total', 'Dens_pers_act_working', 'Dens_pers_act_Nworking', 'Div_total_work', 'Div_act_work', 'Div_socio_work', 'Div_total_Nwork', 'Div_act_Nwork', 'Div_socio_Nwork', 'calidad_cocina', 'diseny_cocina', 'alta_calidad', 'reform_inmob', 'dum_mar_200m', 'dum_ttpp_riel_urb', 'dist_near_riel_km', 'dist_near_viappal_km', 'C_contempo', 'C_estado', 'C_armarios', 'B_contempo', 'B_estado', 'B_lavamano', 'R_contempo', 'R_estado', 'R_carpinte', 'R_singular', 'R_ventana', 'Dum_precio', 'Precio_red', 'scrap_year', 'persona', 'filter_$', 'FAC1_1', 'FAC2_1', 'Muestra_2023', 'EPC_A_emision_2023', 'EPC_B_emision_2023', 'EPC_C_emision_2023', 'EPC_D_emision_2023', 'EPC_E_emision_2023', 'EPC_F_emision_2023', 'EPC_G_emision_2023']\n"
     ]
    }
   ],
   "source": [
    "print(h.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd4df1-2f50-427f-ba12-96695c674508",
   "metadata": {
    "tags": []
   },
   "source": [
    "Si queremos saber si hay alguna celda vacía en nuestro dataframe, ejecutamos el siguiente código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "333e7a39-cd88-4627-ba61-d06ae375c3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc472f-b3a6-4f80-87f7-748a03f85237",
   "metadata": {
    "tags": []
   },
   "source": [
    "Como ya sabemos que sí, las eliminamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "310fa2c3-1a11-492c-adac-bf74b7149c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = h.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab822bd-96ac-44c6-a548-9b1c39dd4352",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2. Definimos la(s) variable(s) a explicar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fd206-7029-4718-a0dc-3541f8468034",
   "metadata": {},
   "source": [
    "Voy a buscar qué columnas son dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f5b78b6-91c7-4659-9c6b-f3e8deaaa74f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type_build', 'Type_opera', 'Climatic_Z', 'numero_aseo', 'ascensor',\n",
       "       'Year_Before_1981', 'Year_1982_2006', 'Year_After_2007',\n",
       "       'grand_terr_20m2', 'superficie_jardin_m2', 'bool_despacho',\n",
       "       'bool_buhardilla', 'bool_trastero', 'bool_lavadero',\n",
       "       'bool_piscina_comunitaria', 'bool_jardin_comunitario', 'bool_amueblado',\n",
       "       'bool_ascensor', 'bool_aire_acondicionado', 'bool_calefaccion',\n",
       "       'bool_chimenea', 'Dum_EPC', 'EPC_A_emision', 'EPC_B_emision',\n",
       "       'EPC_C_emision', 'EPC_D_emision', 'EPC_E_emision', 'EPC_F_emision',\n",
       "       'EPC_G_emision', 'dum_acces_viappal', 'calidad_cocina', 'diseny_cocina',\n",
       "       'alta_calidad', 'reform_inmob', 'dum_mar_200m', 'dum_ttpp_riel_urb',\n",
       "       'C_contempo', 'C_estado', 'C_armarios', 'B_contempo', 'B_estado',\n",
       "       'B_lavamano', 'R_contempo', 'R_estado', 'R_carpinte', 'R_singular',\n",
       "       'R_ventana', 'Dum_precio', 'scrap_year', 'filter_$', 'Muestra_2023',\n",
       "       'EPC_A_emision_2023', 'EPC_B_emision_2023', 'EPC_C_emision_2023',\n",
       "       'EPC_D_emision_2023', 'EPC_E_emision_2023', 'EPC_F_emision_2023',\n",
       "       'EPC_G_emision_2023'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counts = h.nunique()\n",
    "categoricas = unique_counts[unique_counts < 5].index\n",
    "categoricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3fc52-c16e-4148-bba1-312acc3a153a",
   "metadata": {},
   "source": [
    "Noto que desde `C_contempo` hasta `R_ventana` tenemos datos de calidad arquitectónica, por lo que el primer paso será hacer un Análisis de Componentes Principales para disminuir en la medida de la posible estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59c47342-31a2-472e-8aef-7863e0bfab73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_contempo</th>\n",
       "      <th>C_estado</th>\n",
       "      <th>C_armarios</th>\n",
       "      <th>B_contempo</th>\n",
       "      <th>B_estado</th>\n",
       "      <th>B_lavamano</th>\n",
       "      <th>R_contempo</th>\n",
       "      <th>R_estado</th>\n",
       "      <th>R_singular</th>\n",
       "      <th>R_ventana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.771175</td>\n",
       "      <td>0.829585</td>\n",
       "      <td>0.216031</td>\n",
       "      <td>0.812078</td>\n",
       "      <td>0.827633</td>\n",
       "      <td>0.524474</td>\n",
       "      <td>0.782858</td>\n",
       "      <td>0.800836</td>\n",
       "      <td>0.350748</td>\n",
       "      <td>0.330651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.087311</td>\n",
       "      <td>0.175826</td>\n",
       "      <td>0.744095</td>\n",
       "      <td>-0.111461</td>\n",
       "      <td>0.138680</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>-0.103330</td>\n",
       "      <td>0.144193</td>\n",
       "      <td>-0.551318</td>\n",
       "      <td>0.285422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_contempo  C_estado  C_armarios  B_contempo  B_estado  B_lavamano  \\\n",
       "0    0.771175  0.829585    0.216031    0.812078  0.827633    0.524474   \n",
       "1   -0.087311  0.175826    0.744095   -0.111461  0.138680    0.140660   \n",
       "\n",
       "   R_contempo  R_estado  R_singular  R_ventana  \n",
       "0    0.782858  0.800836    0.350748   0.330651  \n",
       "1   -0.103330  0.144193   -0.551318   0.285422  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analyzer import FactorAnalyzer #librería especializada para hacer ACP\n",
    "\n",
    "#Le indicamos en qué columnas se va a centrar\n",
    "df = h[['C_contempo','C_estado','C_armarios','B_contempo','B_estado', 'B_lavamano',\n",
    "           'R_contempo','R_estado','R_singular','R_ventana']]\n",
    "\n",
    "#Le pedimos un ACP con rotación varimax y dos factores (esto ya viene de un análisis de Carlos\n",
    "#El nª de factores se debe analizar previamente.\n",
    "fa = FactorAnalyzer(rotation='varimax', n_factors=2, method='principal')\n",
    "fa.fit(df)\n",
    "\n",
    "rotated_loadings = fa.loadings_\n",
    "\n",
    "# Transponer la matriz de cargas rotadas para que las columnas coincidan con las variables\n",
    "rotated_loadings_transposed = rotated_loadings.T\n",
    "\n",
    "rotated_df = pd.DataFrame(rotated_loadings_transposed, columns=df.columns)\n",
    "\n",
    "rotated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f1545-b73b-4906-85fd-62f50fb18354",
   "metadata": {},
   "source": [
    "Revisando los datos, me doy cuenta que `0` se me acerca a medir el *estado de contemporaneidad y calidad*, mientras que `1`, a medir la *ausencia de elementos singulares*. Los añadimos al dataframe original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a4020f04-c14c-431e-b72e-f4564665cf4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtener los factores extraídos\n",
    "factors = fa.transform(df)\n",
    "\n",
    "# Agregar los factores al DataFrame original\n",
    "h['Estado_contemporaneidad_calidad'] = factors[:, 0]\n",
    "h['Ausencia_singulares_presencia_arm_cocina'] = factors[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871acd9a-31fa-4a4a-8052-c2af99906cd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3. Definimos la variable explicativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c2561-33ef-4c1a-bf09-0bb0743c79ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ahora voy a buscar qué columnas tienen valores *string*, es decir, letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a41a16ac-2559-4693-98ce-cbe81a38fa21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Title', 'Type_build', 'Type_opera', 'Link', 'Location', 'Climatic_Z',\n",
      "       'Nom_Mun', 'descripcion', 'texto_destacado', 'Description',\n",
      "       'calificacion_consumo_letra', 'calificacion_emision_letra', 'persona'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "colstring = h.select_dtypes(include=['object'])\n",
    "print(colstring.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddcf00-5d36-4119-94f3-03b0727c9ac9",
   "metadata": {},
   "source": [
    "Reviso a mayor profundidad qué información tienen aquellas que, creo, pueden acercarme a saber el anuncio de venta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6ff62-b2ca-4460-864b-7a099e8844a8",
   "metadata": {},
   "source": [
    "Ahora que ya sé qué columnas me interesan, las selecciono y empiezo a trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b18d883-11b7-4c22-9ed5-a37db334aeb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto_destacado</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 habitaciones en 11 de setembre</td>\n",
       "      <td>Piso reformado de 4 habitaciones, salón comedo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIS BENET MATEU/ MANUEL DE FALLA</td>\n",
       "      <td>BENET MATEU, PIS D´ORIGEN AMB MOLT BONA DISTRI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apartamento tipo casa Canyelles</td>\n",
       "      <td>Apartamento pero con acceso independiente desd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TODO EXTERIOR Y REFORMADO</td>\n",
       "      <td>[A2977]PISAZO, EL MEJOR DE LA ZONA.FENOMENAL P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102 M2 EXTERIORES CON ASCENSOR</td>\n",
       "      <td>[A3001]VIVIENDA EN LA CALLE GARROFER DE SANT I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>Piso en venta en Can Pantiquet-Riera Seca</td>\n",
       "      <td>Piso En Mollet Del Vallès!Ubicado a 600m de la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>BUENA UBICACIÓN!</td>\n",
       "      <td>La Casa Agency presenta en Exclusividad, esta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>PISO EN PLANTA BAJA CON PATIO DE 60m²</td>\n",
       "      <td>Mis Finques promociona esta planta baja con pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5465</th>\n",
       "      <td>PISO CON TERRAZA</td>\n",
       "      <td>PISO CON TERRAZAPiso con TERRAZA DE 40M2. La v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>Apartamento con 3 habitaciones con ascensor, p...</td>\n",
       "      <td>Precioso piso en una finca joven de tan solo 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5467 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        texto_destacado  \\\n",
       "0                      4 habitaciones en 11 de setembre   \n",
       "1                      PIS BENET MATEU/ MANUEL DE FALLA   \n",
       "2                       Apartamento tipo casa Canyelles   \n",
       "3                             TODO EXTERIOR Y REFORMADO   \n",
       "4                        102 M2 EXTERIORES CON ASCENSOR   \n",
       "...                                                 ...   \n",
       "5462          Piso en venta en Can Pantiquet-Riera Seca   \n",
       "5463                                   BUENA UBICACIÓN!   \n",
       "5464              PISO EN PLANTA BAJA CON PATIO DE 60m²   \n",
       "5465                                   PISO CON TERRAZA   \n",
       "5466  Apartamento con 3 habitaciones con ascensor, p...   \n",
       "\n",
       "                                            Description  \n",
       "0     Piso reformado de 4 habitaciones, salón comedo...  \n",
       "1     BENET MATEU, PIS D´ORIGEN AMB MOLT BONA DISTRI...  \n",
       "2     Apartamento pero con acceso independiente desd...  \n",
       "3     [A2977]PISAZO, EL MEJOR DE LA ZONA.FENOMENAL P...  \n",
       "4     [A3001]VIVIENDA EN LA CALLE GARROFER DE SANT I...  \n",
       "...                                                 ...  \n",
       "5462  Piso En Mollet Del Vallès!Ubicado a 600m de la...  \n",
       "5463  La Casa Agency presenta en Exclusividad, esta ...  \n",
       "5464  Mis Finques promociona esta planta baja con pa...  \n",
       "5465  PISO CON TERRAZAPiso con TERRAZA DE 40M2. La v...  \n",
       "5466  Precioso piso en una finca joven de tan solo 1...  \n",
       "\n",
       "[5467 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colstring[['texto_destacado','Description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d71d9-819a-4a34-be98-f12d937e2ef4",
   "metadata": {},
   "source": [
    "Ahora ya sé que me interesan `Description`, `Estado_contemporaneidad_calidad` y `Ausencia_singulares_presencia_arm_cocina`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b51cd7-8b8e-4953-9f2d-9e8fe56ac9f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.4. Modelo `Estado_contemporaneidad_calidad`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e4a874-b91d-40a9-ba81-fefdef2fe225",
   "metadata": {},
   "source": [
    "Elaboro el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fcb075c8-6feb-4985-8bb7-bcdc0dc9911a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR(kernel='linear')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "\n",
    "# Descargar recursos necesarios para NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Definir función para lematización\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Definir función para eliminar tildes y convertir a minúsculas\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = ''.join(char for char in unicodedata.normalize('NFD', text) if unicodedata.category(char) != 'Mn')  # Eliminar tildes\n",
    "    return text\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(h['Description'], h['Estado_contemporaneidad_calidad'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42) #Ojo: elijo 80-20.\n",
    "\n",
    "# Preprocesamiento y representación de texto\n",
    "custom_stopwords = ['de', 'la', 'el', 'los', 'las', 'en', 'para', \n",
    "                    'con', 'y', 'o', 'un', 'una', 'que', 'se', 'su', 'sus']  # Agrega más palabras si es necesario\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stopwords)\n",
    "\n",
    "# Lematizar y preprocesar el texto\n",
    "X_train_lemmatized = X_train.apply(lemmatize_text)\n",
    "X_test_preprocessed = X_test.apply(preprocess_text)\n",
    "X_test_lemmatized = X_test_preprocessed.apply(lemmatize_text)\n",
    "\n",
    "# Entrenar el vectorizador TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lemmatized)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_lemmatized)\n",
    "\n",
    "# Obtener nombres de características y mapearlos a sus índices\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_index_map = {word: idx for idx, word in enumerate(feature_names)}\n",
    "\n",
    "# Definir diccionario de palabras y factores de multiplicación de peso (eliminando tildes)\n",
    "word_weight_dict = {'noble': 2, 'reformado': 2, 'rehabilitado': 2, 'amplio': 2, \n",
    "                    'equipado': 2, 'restaurada': 2, 'isla': 2, 'isleta': 2, 'estrenar': 2,\n",
    "                    'conservado': 2, 'moderna': 2, 'acondicion': 2, 'lujo': 2, 'muy': 2,\n",
    "                    'diseno': 2, 'estado': 2, 'grand': 2, 'excelente': 2, 'vitroceramica': 2,\n",
    "                    'renovado': 2, 'impecable': 2, 'nuevo': 2, 'perfecto': 2, 'totalmente': 2,\n",
    "                    'total': 2, 'full': 2, 'espectacular': 2, 'maravillosa': 2, 'estupendo': 2,\n",
    "                    'optimo': 2, 'magnifica': 2, 'ideal': 2, 'fantastica': 2, 'vanguardia': 2,\n",
    "                    'office': 2, 'americana': 2, 'abierto': 2, 'vistas': 2, 'luminoso': 2,\n",
    "                    'iluminada': 2, 'bien': 2, 'entrar': 2, 'actualizado': 2, 'conservacion': 2,\n",
    "                    'buen': 2, 'condicion': 2, 'mantenida': 2, 'cuidado': 2, 'precioso': 2,\n",
    "                    'bonita': 2, 'encanto': 2, 'magnifica/co': 2, 'senoral': 2, 'vista': 2,\n",
    "                    'panoramico': 2, 'exterior': 2, 'modernista': 2, 'acogedora': 2, 'regia': 2,\n",
    "                    'gusto': 2, 'noucentista': 2, 'exclusivo': 2, 'neo-clasica': 2,\n",
    "                    'neoclasica': 2, 'inmejorable': 2, 'fabuloso': 2, 'majestuosa': 2,\n",
    "                    'alta calidad': 2, 'alto standing': 2, 'super': 2, 'impresionante': 2,\n",
    "                    'elegante': 2, 'esplendido': 2, 'calida': 2, 'especial': 2}\n",
    "\n",
    "\n",
    "# Modificar pesos según el diccionario\n",
    "for word, weight_factor in word_weight_dict.items():\n",
    "    word_no_accents = preprocess_text(word)\n",
    "    if word_no_accents in feature_index_map:\n",
    "        word_index = feature_index_map[word_no_accents]\n",
    "        X_train_tfidf[:, word_index] *= weight_factor\n",
    "        X_test_tfidf[:, word_index] *= weight_factor\n",
    "\n",
    "# Entrenar un modelo de regresión (por ejemplo, SVR)\n",
    "svm_regressor = SVR(kernel='linear')\n",
    "svm_regressor.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b1718a4-ea6b-403a-b0bf-904dc57669ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of Determination (R^2): 0.29755162026480875\n",
      "Mean Absolute Error (MAE): 0.7034394507861268\n",
      "Relative Absolute Error (RAE): 111.62444628676472\n",
      "Root Mean Squared Error (RMSE): 0.8540028444553915\n",
      "Root Relative Squared Error (RRSE): 0.8375775242400891\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_predSVM = svm_regressor.predict(X_test_tfidf)\n",
    "\n",
    "# Coeficiente de Determinación (R^2)\n",
    "r2SVM = r2_score(y_test, y_predSVM)\n",
    "# Error Absoluto Medio (MAE)\n",
    "maeSVM = mean_absolute_error(y_test, y_predSVM)\n",
    "# Error Absoluto Relativo (RAE)\n",
    "mean_y_test = y_test.mean()\n",
    "raeSVM = maeSVM / mean_y_test\n",
    "# Raíz del Error Cuadrático Medio (RMSE)\n",
    "rmseSVM = np.sqrt(mean_squared_error(y_test, y_predSVM))\n",
    "# Raíz del Error Cuadrático Medio Relativo (RRSE)\n",
    "std_y_test = y_test.std()\n",
    "rrseSVM = rmseSVM / std_y_test\n",
    "\n",
    "\n",
    "print(\"Coefficient of Determination (R^2):\", r2SVM)\n",
    "print(\"Mean Absolute Error (MAE):\", maeSVM)\n",
    "print(\"Relative Absolute Error (RAE):\", raeSVM)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmseSVM)\n",
    "print(\"Root Relative Squared Error (RRSE):\", rrseSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050d861-a134-4065-812c-dd3d3b80c8e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.5. Modelo `Ausencia_singulares_presencia_arm_cocina`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bcb7ef81-ad47-4b49-a40b-ea54b35c217a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR(kernel='linear')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "\n",
    "# Descargar recursos necesarios para NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Definir función para lematización\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Definir función para eliminar tildes y convertir a minúsculas\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = ''.join(char for char in unicodedata.normalize('NFD', text) if unicodedata.category(char) != 'Mn')  # Eliminar tildes\n",
    "    return text\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train2, y_test2 = train_test_split(h['Description'], h['Ausencia_singulares_presencia_arm_cocina'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42) #Ojo: elijo 80-20.\n",
    "\n",
    "# Preprocesamiento y representación de texto\n",
    "custom_stopwords = ['de', 'la', 'el', 'los', 'las', 'en', 'para', \n",
    "                    'con', 'y', 'o', 'un', 'una', 'que', 'se', 'su', 'sus']  # Agrega más palabras si es necesario\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stopwords)\n",
    "\n",
    "# Lematizar y preprocesar el texto\n",
    "X_train_lemmatized = X_train.apply(lemmatize_text)\n",
    "X_test_preprocessed = X_test.apply(preprocess_text)\n",
    "X_test_lemmatized = X_test_preprocessed.apply(lemmatize_text)\n",
    "\n",
    "# Entrenar el vectorizador TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_lemmatized)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_lemmatized)\n",
    "\n",
    "# Obtener nombres de características y mapearlos a sus índices\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_index_map = {word: idx for idx, word in enumerate(feature_names)}\n",
    "\n",
    "# Definir diccionario de palabras y factores de multiplicación de peso (eliminando tildes)\n",
    "word_weight_dict = {'noble': 2, 'reformado': 2, 'rehabilitado': 2, 'amplio': 2, \n",
    "                    'equipado': 2, 'restaurada': 2, 'isla': 2, 'isleta': 2, 'estrenar': 2,\n",
    "                    'conservado': 2, 'moderna': 2, 'acondicion': 2, 'lujo': 2, 'muy': 2,\n",
    "                    'diseno': 2, 'estado': 2, 'grand': 2, 'excelente': 2, 'vitroceramica': 2,\n",
    "                    'renovado': 2, 'impecable': 2, 'nuevo': 2, 'perfecto': 2, 'totalmente': 2,\n",
    "                    'total': 2, 'full': 2, 'espectacular': 2, 'maravillosa': 2, 'estupendo': 2,\n",
    "                    'optimo': 2, 'magnifica': 2, 'ideal': 2, 'fantastica': 2, 'vanguardia': 2,\n",
    "                    'office': 2, 'americana': 2, 'abierto': 2, 'vistas': 2, 'luminoso': 2,\n",
    "                    'iluminada': 2, 'bien': 2, 'entrar': 2, 'actualizado': 2, 'conservacion': 2,\n",
    "                    'buen': 2, 'condicion': 2, 'mantenida': 2, 'cuidado': 2, 'precioso': 2,\n",
    "                    'bonita': 2, 'encanto': 2, 'magnifica/co': 2, 'senoral': 2, 'vista': 2,\n",
    "                    'panoramico': 2, 'exterior': 2, 'modernista': 2, 'acogedora': 2, 'regia': 2,\n",
    "                    'gusto': 2, 'noucentista': 2, 'exclusivo': 2, 'neo-clasica': 2,\n",
    "                    'neoclasica': 2, 'inmejorable': 2, 'fabuloso': 2, 'majestuosa': 2,\n",
    "                    'alta calidad': 2, 'alto standing': 2, 'super': 2, 'impresionante': 2,\n",
    "                    'elegante': 2, 'esplendido': 2, 'calida': 2, 'especial': 2}\n",
    "\n",
    "\n",
    "# Modificar pesos según el diccionario\n",
    "for word, weight_factor in word_weight_dict.items():\n",
    "    word_no_accents = preprocess_text(word)\n",
    "    if word_no_accents in feature_index_map:\n",
    "        word_index = feature_index_map[word_no_accents]\n",
    "        X_train_tfidf[:, word_index] *= weight_factor\n",
    "        X_test_tfidf[:, word_index] *= weight_factor\n",
    "\n",
    "# Entrenar un modelo de regresión (por ejemplo, SVR)\n",
    "svm_regressor2 = SVR(kernel='linear')\n",
    "svm_regressor2.fit(X_train_tfidf, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c0389600-0f7f-4add-bc5f-813cccbdde42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of Determination (R^2): -0.02058770542835453\n",
      "Mean Absolute Error (MAE): 0.7353649888431517\n",
      "Relative Absolute Error (RAE): -74.10091606555228\n",
      "Root Mean Squared Error (RMSE): 1.0292859489534427\n",
      "Root Relative Squared Error (RRSE): 1.009585195439135\n"
     ]
    }
   ],
   "source": [
    "# Predecir en el conjunto de prueba\n",
    "y_predSVM2 = svm_regressor2.predict(X_test_tfidf)\n",
    "\n",
    "# Coeficiente de Determinación (R^2)\n",
    "r2SVM2 = r2_score(y_test2, y_predSVM)\n",
    "# Error Absoluto Medio (MAE)\n",
    "maeSVM2 = mean_absolute_error(y_test2, y_predSVM2)\n",
    "# Error Absoluto Relativo (RAE)\n",
    "mean_y_test2 = y_test2.mean()\n",
    "raeSVM2 = maeSVM2 / mean_y_test2\n",
    "# Raíz del Error Cuadrático Medio (RMSE)\n",
    "rmseSVM2 = np.sqrt(mean_squared_error(y_test2, y_predSVM2))\n",
    "# Raíz del Error Cuadrático Medio Relativo (RRSE)\n",
    "std_y_test2 = y_test2.std()\n",
    "rrseSVM2 = rmseSVM2 / std_y_test2\n",
    "\n",
    "\n",
    "print(\"Coefficient of Determination (R^2):\", r2SVM2)\n",
    "print(\"Mean Absolute Error (MAE):\", maeSVM2)\n",
    "print(\"Relative Absolute Error (RAE):\", raeSVM2)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmseSVM2)\n",
    "print(\"Root Relative Squared Error (RRSE):\", rrseSVM2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532dbb8-1828-427f-a2ca-d09cbf2ae883",
   "metadata": {},
   "source": [
    "Algunos apuntes: \n",
    "\n",
    "- Precisión (Precision): La precisión se refiere a la proporción de las instancias clasificadas como positivas que son realmente positivas.Se calcula como el número de verdaderos positivos dividido por la suma de verdaderos positivos y falsos positivos. **Es útil cuando el costo de los falsos positivos es alto y deseas minimizarlos**.\n",
    "\n",
    "- Exactitud (Accuracy): La exactitud es la proporción de todas las predicciones que son correctas. Se calcula como la suma de verdaderos positivos y verdaderos negativos dividido por el total de instancias. Es una medida global del rendimiento del modelo y **es útil cuando todas las clases tienen una importancia similar**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1690d2e7-e302-41cb-9713-9f228ad278e6",
   "metadata": {},
   "source": [
    "Guardamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2a137-bef5-4eed-a904-7c20a3f73541",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61962f08-4730-460a-a034-1f0690e29986",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1. Modelo `Estado_contemporaneidad_calidad`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d07f36-b153-45ad-bee7-64bf0cd76fe9",
   "metadata": {},
   "source": [
    "Esta vez usaremos una red neuronal con tres capas densas, cada una seguida de una capa de abandono (dropout) para evitar el sobreajuste. Utilizamos la función de activación 'relu' en las capas ocultas. El optimizador Adam se utiliza para minimizar la pérdida de entropía cruzada binaria, y la exactitud (*accuracy*) se utiliza como métrica de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7e0994bd-4336-464e-ae53-6198c5404146",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Dropout\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "##### Esto ya lo tengo del modelo anterior, por eso no lo ejecuto\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "#X = habit['texto_destacado']\n",
    "#y = habit['alta_calidad']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Representación TF-IDF de las características de texto\n",
    "#tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stopwords)\n",
    "#X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "#X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "##### A partir de aquí ya son cosas nuevas para este modelo de redes neuronales\n",
    "# Escalar las características numéricas\n",
    "#scaler = StandardScaler()\n",
    "#X_train_scaled = scaler.fit_transform(X_train_tfidf.toarray())\n",
    "#X_test_scaled = scaler.transform(X_test_tfidf.toarray())\n",
    "\n",
    "# Crear el modelo de redes neuronales\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1) #Sigmoid es más adecuado para variables binarias\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(lr=0.005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "historyANN = model.fit(X_train_scaled, y_train, #Son las mismas del modelo anterior\n",
    "                    epochs=500, batch_size=32, validation_data=(X_test_scaled, y_test),\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2430fe-ea03-4573-99ca-25cd03078a8f",
   "metadata": {},
   "source": [
    "Encuentro los parámetros para comparar con modelo SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "98e45c2d-07a9-461f-95ab-7b564bd81d06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [770, 3076]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m y_predANN \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_tfidf)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calcular Coefficient of Determination (R2)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m r2ANN \u001b[38;5;241m=\u001b[39m r2_score(y_test, y_predANN)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Calcular Mean Absolute Error (MAE)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m maeANN \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, y_predANN)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:989\u001b[0m, in \u001b[0;36mr2_score\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    849\u001b[0m     {\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m     force_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    869\u001b[0m ):\n\u001b[0;32m    870\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\":math:`R^2` (coefficient of determination) regression score function.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m    Best possible score is 1.0 and it can be negative (because the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03m    -inf\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    990\u001b[0m         y_true, y_pred, multioutput\n\u001b[0;32m    991\u001b[0m     )\n\u001b[0;32m    992\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y_pred) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m    100\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    101\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [770, 3076]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Hacer predicciones con el modelo entrenado\n",
    "y_predANN = model.predict(X_train_tfidf)\n",
    "\n",
    "# Calcular Coefficient of Determination (R2)\n",
    "r2ANN = r2_score(y_test, y_predANN)\n",
    "\n",
    "# Calcular Mean Absolute Error (MAE)\n",
    "maeANN = mean_absolute_error(y_test, y_predANN)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Coefficient of Determination (R2):\", r2ANN)\n",
    "print(\"Mean Absolute Error (MAE):\", maeANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae3133-9912-4eee-be76-e6d308323397",
   "metadata": {},
   "source": [
    "Es tan malo que el R2 sale ¡negativo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b55c4010-a257-4b58-8228-0177579bf078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 3ms/step\n",
      "Coefficient of Determination (R2): -2.532922164660907\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'maeANN2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Imprimir los resultados\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoefficient of Determination (R2):\u001b[39m\u001b[38;5;124m\"\u001b[39m, r2ANN2)\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Absolute Error (MAE):\u001b[39m\u001b[38;5;124m\"\u001b[39m, maeANN2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'maeANN2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "##### Esto ya lo tengo del modelo anterior, por eso no lo ejecuto\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "#X = habit['texto_destacado']\n",
    "#y = habit['alta_calidad']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Representación TF-IDF de las características de texto\n",
    "#tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stopwords)\n",
    "#X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "#X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "##### A partir de aquí ya son cosas nuevas para este modelo de redes neuronales\n",
    "# Escalar las características numéricas\n",
    "#scaler = StandardScaler()\n",
    "#X_train_scaled = scaler.fit_transform(X_train_tfidf.toarray())\n",
    "#X_test_scaled = scaler.transform(X_test_tfidf.toarray())\n",
    "\n",
    "# Crear el modelo de redes neuronales\n",
    "model2 = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1) #Sigmoid es más adecuado para variables binarias\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model2.compile(optimizer=Adam(lr=0.005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "historyANN2 = model.fit(X_train_scaled, y_train2, #Son las mismas del modelo anterior\n",
    "                    epochs=500, batch_size=32, validation_data=(X_test_scaled, y_test),\n",
    "                    verbose=False)\n",
    "\n",
    "# Hacer predicciones con el modelo entrenado\n",
    "y_predANN2 = model2.predict(X_test_scaled)\n",
    "\n",
    "# Calcular Coefficient of Determination (R2)\n",
    "r2ANN2 = r2_score(y_test2, y_predANN2)\n",
    "\n",
    "# Calcular Mean Absolute Error (MAE)\n",
    "maeANN2 = mean_absolute_error(y_test2, y_predANN2)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Coefficient of Determination (R2):\", r2ANN2)\n",
    "print(\"Mean Absolute Error (MAE):\", maeANN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ee7b7-5d9d-4494-9745-0b6d5a595292",
   "metadata": {},
   "source": [
    "Sigue siendo bastante malo el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811e9a2-f670-4daf-8efc-873c842c8e2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Random Forest (6k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa6b12-3d79-4ead-bf5b-11771075c1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44ccf4ec-4154-4f1d-9675-596bb4a03ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.823583180987203\n",
      "Recall: 0.04081632653061224\n",
      "Precision: 0.6153846153846154\n",
      "F1 Score: 0.07655502392344496\n",
      "Confusion Matrix:\n",
      " [[893   5]\n",
      " [188   8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Crear y entrenar el modelo RandomForestRegressor\n",
    "random_forest_reg = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "random_forest_reg.fit(X_train_scale, y_train)\n",
    "\n",
    "# Hacer predicciones con el modelo entrenado\n",
    "y_pred_rf = random_forest_reg.predict(X_train_scale)\n",
    "\n",
    "# Calcular Coefficient of Determination (R2)\n",
    "r2_rf_reg = r2_score(y_test, y_pred_rf_reg)\n",
    "\n",
    "# Calcular Mean Absolute Error (MAE)\n",
    "mae_rf_reg = mean_absolute_error(y_test, y_pred_rf_reg)\n",
    "\n",
    "# Calcular Root Mean Squared Error (RMSE)\n",
    "rmse_rf_reg = np.sqrt(mean_squared_error(y_test, y_pred_rf_reg))\n",
    "\n",
    "# Calcular Relative Absolute Error (RAE)\n",
    "rae_rf_reg = np.mean(np.abs(y_pred_rf_reg - y_test)) / np.mean(np.abs(y_test - np.mean(y_test)))\n",
    "\n",
    "# Calcular Root Relative Squared Error (RRSE)\n",
    "rrse_rf_reg = np.sqrt(np.sum(np.square(y_pred_rf_reg - y_test))) / np.sqrt(np.sum(np.square(y_test - np.mean(y_test))))\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Coefficient of Determination (R2):\", r2_rf_reg)\n",
    "print(\"Mean Absolute Error (MAE):\", mae_rf_reg)\n",
    "print(\"Relative Absolute Error (RAE):\", rae_rf_reg)\n",
    "print(\"Root Relative Squared Error (RRSE):\", rrse_rf_reg)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_rf_reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c2d43-ea76-49ca-bc1b-30f2e0449b26",
   "metadata": {},
   "source": [
    "# 4. Comparamos los resultados de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56aa2ed9-a9b9-4efa-825c-e22c9db3d0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Modelo  Accuracy  Precision    Recall  F1 Score\n",
      "0    SVM  0.823583   0.588235  0.051020  0.093897\n",
      "1    ANN  0.813528   0.409091  0.091837  0.150000\n",
      "2     RF  0.823583   0.615385  0.040816  0.076555\n"
     ]
    }
   ],
   "source": [
    "# Crear un diccionario con los datos\n",
    "data = {\n",
    "    'Modelo': ['SVM', 'ANN', 'RF'],\n",
    "    'Accuracy': [accuracySVM, accuracyANN, accuracyRF],\n",
    "    'Precision': [precisionSVM, precisionANN, precisionRF],\n",
    "    'Recall': [recallSVM, recallANN, recallRF],\n",
    "    'F1 Score': [f1SVM, f1ANN, f1RF]\n",
    "}\n",
    "\n",
    "# Crear un DataFrame con los datos\n",
    "comparacion = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar la tabla\n",
    "print(comparacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2cc10-d03e-48f6-8feb-1e2cc8434b4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Precisión (Accuracy):\n",
    "    - La precisión es la proporción de predicciones correctas sobre el total de predicciones.\n",
    "    - Una precisión del 1.0 indica que todas las predicciones son correctas, mientras que una precisión del 0.0 indica que ninguna predicción es correcta.\n",
    "    - Es una métrica general del rendimiento del modelo, pero puede ser engañosa si hay un desequilibrio en las clases objetivo.\n",
    "\n",
    "- Recall (Exhaustividad):\n",
    "    - La exhaustividad es la proporción de positivos reales que se identificaron correctamente.\n",
    "    - Una exhaustividad del 1.0 indica que todas las instancias positivas se han identificado correctamente, mientras que una exhaustividad del 0.0 indica que ninguna instancia positiva se ha identificado correctamente.\n",
    "    - Es útil cuando la identificación de instancias positivas es crítica y no se pueden permitir falsos negativos.\n",
    "\n",
    "- Precisión (Precision):\n",
    "    - La precisión es la proporción de instancias positivas predichas que fueron correctamente identificadas.\n",
    "    - Una precisión del 1.0 indica que todas las instancias predichas como positivas son verdaderas positivas, mientras que una precisión del 0.0 indica que ninguna instancia predicha como positiva es realmente positiva.\n",
    "    - Es útil cuando es importante evitar falsos positivos.\n",
    "\n",
    "- F1 Score:\n",
    "    - El puntaje F1 es la media armónica de precisión y exhaustividad.\n",
    "    - Proporciona un equilibrio entre precisión y exhaustividad, lo que lo hace útil cuando se desea tener un buen rendimiento en ambas métricas.\n",
    "    - Un puntaje F1 del 1.0 indica un equilibrio perfecto entre precisión y exhaustividad.\n",
    "\n",
    "- Matriz de Confusión:\n",
    "    - La matriz de confusión es una tabla que describe la calidad de las predicciones del modelo.\n",
    "    - Proporciona una descripción detallada de los resultados de clasificación, mostrando el número de verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos.\n",
    "    - Es útil para identificar dónde el modelo está cometiendo errores y para evaluar el desempeño en cada clase por separado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
